---
output:
  pdf_document: default
  html_document: default
---
# Day of Week Prediction Power of Expenditures Analysis
## Nicholas Hartman Ponce
### nicholas.paul.hartman@gmail.com
### [Github Repo](https://github.com/kingofharts/Expenditure-Analysis)

```{r echo = TRUE}
# This R markdown file contains the necessary markdown, commentary, and code to produce a knitr'd presentation my analysis of my wife and I's US Dollar Expenditure Data from ~April 2014 to ~July 2018.

# This file assumes that all files in the github repo are available in your R working directory.

# This analysis is intended to provide a demonstrative sample of my ability to conduct and narrate analysis using self-developed institutional knowledge, Excel, and R.
```
---

## Context

My wife and I recently discussed differences between weekday and weekend spending.  I had already plotted average spend by day-of-week, but I began to ask myself whether the expenditure data held any predictive value.

As we are repatriating to the US in August, and I am currently seeking work in analysis in Providence, RI, and demonstrable analytical ability is helpful to that pursuit, a small investigation seemed appropriate - and so here we are.

---

## Premise

My wife and I, both early/mid-career, white-collar professionals, have generally adhered to the Monday - Friday workweek conventional in the west.  As most people struggle to spend money during employment hours (excepting generally small-dollar lunches, coffee, etc.), my hunch is our daily expenditures are lower during the week than at the weekend.

The question here then is whether I can predict either:

- Weekday from Weekend, or
- Which specific day of the week

using sum/count of our various expenditures for a given day.

---

## Plan

To get to either A) a working predictive model or B) a determination that that would require more attributes, I'll:

- consider what's available
- decide what to export, and whether to aggregate before or after export
- export to .tld and then repackage/reformat the data in R
- explore/evaluate and plot the data
- index & split the data
- train a couple of models
- test, see if anything works
- summarize findings

---

## Data, Broadly

I have recorded each of our US Dollar financial transactions from ~April 2014.  The columns/attributes have multiplied over time, but the core of Date, Local Time to the quarter hour, Vendor, and Dr/Cr amounts have not changed.

While I have recorded all of our USD transactions, for this analysis I will be looking only at expenditures, excluding transfers between accounts, credit card payments, etc.

Also filtered out from the data will be expenditures that, for one reason or another, are clearly not relevant to DoW analysis (i.e. rent payments, loan payments, etc. which fall on a fixed calendar date, among other items).

---

## Data, Basic Characteristics

The source data are ~7,900 rows (1 Apr 2014 through 12 Jul 2018) across 19 columns of USD transactions.

The transactions are only those in USD because we have spent the majority of our time living and working in the US using the USD, our non-USD expenditures comprise a small minority (both by count and sum) of our total expenditures, and, while I have plans for other uses of these data, tracking historical Point-in-Time exchange rates is not one of them.

A data dictionary for the source is provided as "SourceDataDictionary.txt".  It uses column names as formatted in Excel, but the sequence of columns is unchanged from below.  Here are the columns as dumped raw to R:

```{r echo=TRUE}

## Read in precanned Column Names

AllColNames <- read.delim("AllColNames.txt")

AllColNames

```

For this analysis I've decided to limit use to four: Date, DoW, Debit, and Credit.  Date will be used to aggregate, rather than for prediction (pretty easy to predict DoW if you know what the date is).

---

## Data, Quality

While I'm certain the data are not 100% perfect, I am similarly certain that the error rate is negligibly small.

Informed by professional habits, I've been disciplined about data entry habits/controls (various account rec's, etc., personal month-end closes, etc.) such that errors, where they exist, are not material.

Known errors are generally something like, "I spent $15 in cash, but how much was at the Food Truck and how much was the bottle of water at the kiosk?  Can't remember, put it all in as Food Truck."

There are a number of blank values across a handful of columns I'm working to backfill in separate efforts, however none of these columns are used in this analysis.

---

## Initial Excel Decisions

As mentioned above, not all transactions in the data will be relevant and/or useful for this analysis, and the goal is to work from expenditure Sum-by-Day, not individual transactions.  So there's some filtering and some aggregating to do.

It's not strictly necessary to do either in one or the other of Excel or R.  Seeing as the pre-export data already live in Excel with easily GUI-filtered columns, it'd probably be easiest to just do that pre-export.

As far as aggregation, I could either pivot my way there and then export, or use a simple script.  In R, the aggregation code would look something like:

```{r echo=TRUE}

## Read in disaggregated Transactions
Demo <- read.delim("DataExp1.txt")

## Change class on Date and DoW
Demo[,1] <- as.factor(Demo[,1])
Demo[,2] <- as.factor(Demo[,2])

## Replace nulls with 0
Demo[is.na(Demo)] <- 0

## First net Debits and Credits
Demo$Net <- Demo$Debit + Demo$Credit

## Aggregate transaction lines to Sum-by-Date
AggDemo <- aggregate(x = Demo$Net, by = list(DoW = Demo$DoW.M.1, DatesList = Demo$Date), FUN = "sum")

## Clean up names
names(AggDemo) <- c("DoW","Date","NetExp")

## Review
head(AggDemo)

```

That's not terrible, but this kind of work is so well developed in Excel, no point in reinventing the wheel.  I'll export the filtered, pivoted/aggregated data.

Actually, just before exporting, I can even quickly and easily handle the Workday/Weekend day column I'll need with with a simple IF formula column:

=IF(*DoW* < 6, TRUE, FALSE)

Where *DoW* in Excel is the relevant cell reference.

Great, so now we've got what we want out of Excel (DataExp2.txt).  Let's see what we can find out.

---

## Preliminary Analysis

We'll begin by bringing every thing, cleaning/reformatting it, and tidying up the environment as we go:

```{r echo = FALSE, results = 'hide', message = FALSE}
# Load Libraries
library(caret)
library(ellipse)
library(e1071)
library(randomForest)
library(plyr)
library(sp)
library(ggplot2)
```
```{r echo=TRUE}
# Load Data
RawImport <- read.delim("DataExp2.txt", header = TRUE)

# Clean Data
CleanImport <- RawImport

CleanImport[,1] <- as.factor(CleanImport[,1])
CleanImport[,2] <- as.factor(CleanImport[,2])

CleanImport[,5] <- CleanImport[,5]*-1

colnames(CleanImport)[1] <- "Date"
colnames(CleanImport)[2] <- "DoW"
colnames(CleanImport)[5] <- "Exp"

drops <- c("Date")
CleanImport <- CleanImport[ , !(names(CleanImport) %in% drops)]

remove(RawImport,drops)

head(CleanImport)
```

Great!  Now we've got one, clean R data frame:

```{r echo = TRUE}
dim(CleanImport)
summary(CleanImport)
```

4 columns of 1,561 rows to work with.  Let's make a quick visual inspection of the data, starting with Net Expenditure by Day of the Week:

```{r echo = TRUE}
ExpByDoWBox <- ggplot(CleanImport, aes(x = DoW, y = Exp)) + geom_boxplot()
ExpByDoWBox
```

So definitely skewed, lots of outliers, but not really surprising, since it's intuitive that there would be many fewer sub-zero expenditure days than high-spend days.

Some cause for prediction concern for the weekdays; Monday through Thursday's boxes are pretty similar.  On the other hand, some cause for hope on the Weekday/Weekend front, although Friday could throw a wrench in that.  Let's take a look at transaction counts:

```{r echo = TRUE}
CountByDoWBox <- ggplot(CleanImport, aes(x = DoW, y = Count)) + geom_boxplot()
CountByDoWBox
```

Hmm.  So still a bit of a difference between the week and the weekend, but neither substantial nor really distinct.  Let's try one other approach:

```{r echo = TRUE}
x <- as.matrix(CleanImport[,c(3:4)])
y <- factor(CleanImport$DoW)
featurePlot(x, y, plot = "ellipse")
```

Still not much separation.  One last look:

```{r echo = TRUE}
x <- as.matrix(CleanImport[,c(3:4)])
y <- factor(CleanImport$Weekday)
featurePlot(x, y, plot = "ellipse")
```

Uf, not looking good just on these variables.  We can see some of the higher Weekend values (blue) scattering out to the top/right, but we've still got a lot of overlap with Weekdays.

---

## Statistical Review

An ANOVA on the data (Count/Exp vs. DoW) seem the next logical step, but we should probably check the distributions first before settling on that.  Let's look at this another way to get a sense how off of Normal the distributions are:

```{r echo = TRUE}
## Arrange Data for Plotting
x <- as.matrix(CleanImport[,c(3:4)])
y <- factor(CleanImport$DoW)

## Plot
featurePlot(x, y, plot = "density", 
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free"))
            )
```

So we can see the skew, but it's not multi-modal or otherwise bizarrely shaped.  Let's get a sense of the distribution of the residuals:

```{r echo = TRUE}
## Setup Residuals Matrix
Residuals <- matrix(nrow = 1561, ncol = 2)

## Populate Residuals
Residuals[,1] <- resid(aov(Exp ~ DoW, CleanImport))
Residuals[,2] <- resid(aov(Count ~ DoW, CleanImport))

## Tee-up DoW factors
Variable <- factor(CleanImport$DoW)

## Plot Residuals
featurePlot(Residuals, Variable, plot = "density", 
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")))
```

Better, but clearly not normal.  Some skew, and some inconsistent peaks.

So with non-Normal residuals distributions, one-way ANOVA evaluation of the DoW factors is out.  